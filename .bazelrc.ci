startup --host_jvm_args=-Dbazel.DigestFunction=sha256

# https://docs.github.com/en/actions/using-github-hosted-runners/about-github-hosted-runners#supported-runners-and-hardware-resources
# Limit resources since GitHub Actions VMs have 2 cores and 2GB of ram
build --local_cpu_resources=2
build --local_ram_resources=5000
build --worker_max_instances=2

# Ensure sandboxing is on to increase hermeticity.
build --spawn_strategy=sandboxed

build --compilation_mode=opt

# This is so we understand failures better
build --verbose_failures
build --worker_verbose
test --test_output=errors
test --test_verbose_timeout_warnings

# Enable remote caching. Remote cache is implemented as 
# a simple GCS bucket, which is publicly readable but can only be written to
# on commits to main, for which we assume trust, unlike PR's and
# other anonymous builds.
# The cache is available here: https://storage.googleapis.com/livegrep-bazel-remote-cache. 
# Having the cache publicly readable allows PRs and local developers to use it for their builds. 

# Favoring a remote cache rather than the GH actions cache because the
# GH actions cache doesn't support updating the cache content for a key because
# caches are immutable, which renders what Bazel does best moot. For example,
# if we create a cache entry with a key linux-HASH(WORKSPACE) and create a build,
# the initial build contents will be cached. But for ANY subsequent runs that
# don't update the WORKSPACE file (which is pretty much all of them) bazel can't
# add any artifcats to the cache, which destroys the build times of newer code!

# Another cool side effect of enabling/using a remote cache is that eventually 
# we can spread out our CI to run `bazel build` for multiple OS's, which will
# seed the cache with the build outputs of multiple OSs, allowing
# most developers to use the remote cache on their local machines. Not done now
# because some work needs to be done to create cache_keys based on the system
# that bazel is running on, so that we don't poison the cache/get false cache
# hits.
# See: https://github.com/bazelbuild/bazel/issues/4558
# And a sample implementation: https://github.com/kubernetes/test-infra/blob/master/images/bootstrap/create_bazel_cache_rcs.sh#L45

# If you'd like to set up a remote build cache for your fork:
# 1. Set up GCS cache: https://docs.bazel.build/versions/main/remote-caching.html#google-cloud-storage
# 2. Add a GCP_SVC_ACCT_CREDS repository secret that has the service account key
#    that you created in step 1.

common --remote_cache=https://storage.googleapis.com/livegrep-bazel-remote-cache
common --remote_timeout=300s
# don't fail if the cache is unavailable
common --remote_local_fallback=true
# remote upload defaults to true, which we don't want. We use sed to switch
# false to true during the CI build if necessary
common --remote_upload_local_results=false
# default is 100, setting to 0 lets Bazel choose the number of tcp connections
# to handle
common --remote_max_connections=0

# For clarity, the following options are configured at CI run time depending on
# whether or not we trust the commit. (push vs pr) & creds available
# common --remote_upload_local_results=true
# --google_credentials=some_file
